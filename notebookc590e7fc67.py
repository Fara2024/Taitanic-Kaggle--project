{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# 1. Importing Libraries for Data Handling\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:22.346243Z\",\"iopub.execute_input\":\"2025-10-18T09:30:22.346547Z\",\"iopub.status.idle\":\"2025-10-18T09:30:22.663411Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:22.346526Z\",\"shell.execute_reply\":\"2025-10-18T09:30:22.662483Z\"}}\nimport numpy as np \nimport pandas as pd\n\n# %% [markdown]\n# pandas (pd): Used for loading, manipulating, and analyzing structured data (like CSVs or Excel files).\n# \n# numpy (np): Used for numerical operations, arrays, and mathematical computations.\n\n# %% [markdown]\n# 2. Importing Libraries for Visualization\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:22.664715Z\",\"iopub.execute_input\":\"2025-10-18T09:30:22.665127Z\",\"iopub.status.idle\":\"2025-10-18T09:30:23.573196Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:22.665093Z\",\"shell.execute_reply\":\"2025-10-18T09:30:23.572474Z\"}}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# %% [markdown]\n# seaborn (sns): High-level visualization library built on matplotlib; great for statistical plots.\n# \n# matplotlib.pyplot (plt): Basic plotting library for line charts, scatter plots, histograms, etc.\n# \n# %matplotlib inline: Jupyter Notebook magic command that ensures plots are displayed inside the notebook.\n\n# %% [markdown]\n# 3. Data Preprocessing Tools\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:23.573996Z\",\"iopub.execute_input\":\"2025-10-18T09:30:23.574316Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.000649Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:23.574296Z\",\"shell.execute_reply\":\"2025-10-18T09:30:23.999761Z\"}}\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# %% [markdown]\n# OrdinalEncoder: Converts categorical variables to integers based on order (e.g., 'low', 'medium', 'high' ‚Üí 0, 1, 2).\n# \n# OneHotEncoder: Converts categorical variables into binary columns (e.g., 'red', 'blue', 'green' ‚Üí [1,0,0], [0,1,0], [0,0,1]).\n# \n# SimpleImputer: Fills missing values in your dataset (e.g., mean, median, mode, or a constant value).\n\n# %% [markdown]\n# 4. Column Transformations and Pipelines\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.002790Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.003199Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.026984Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.003176Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.026187Z\"}}\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n# %% [markdown]\n# ColumnTransformer / make_column_transformer: Apply different preprocessing steps to different columns. For example, numeric columns can be scaled and categorical columns can be encoded.\n# \n# Pipeline / make_pipeline: Combine preprocessing + model into a single workflow. This makes it easy to train, evaluate, and deploy models without manually applying each step.\n\n# %% [markdown]\n# 5. Machine Learning Models\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.027709Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.027958Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.436761Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.027921Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.435979Z\"}}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n\n\n# %% [markdown]\n# LogisticRegression: Classification using a linear decision boundary.\n# \n# SVC (Support Vector Classifier): Tries to find the optimal hyperplane that separates classes.\n# \n# DecisionTreeClassifier: Tree-based model that splits data based on feature thresholds.\n# \n# KNeighborsClassifier: Classifies based on the majority class of k nearest neighbors.\n# \n# GaussianNB: Naive Bayes classifier assuming Gaussian distribution of features.\n# \n# XGBClassifier: Gradient Boosted Trees implementation from XGBoost, very powerful for tabular data.\n# \n# Ensemble Models (RandomForest, AdaBoost, GradientBoosting, ExtraTrees, VotingClassifier): Combine multiple models to improve accuracy.\n# \n# VotingClassifier: Combines predictions from multiple classifiers by majority vote (or weighted vote).\n\n# %% [markdown]\n# 6. Model Selection & Evaluation Tools\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.437909Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.438301Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.442299Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.438273Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.441489Z\"}}\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, GridSearchCV\n\n\n# %% [markdown]\n# train_test_split: Split dataset into training and testing sets.\n# \n# cross_val_score: Perform cross-validation to evaluate model performance.\n# \n# StratifiedKFold: Ensures each fold in cross-validation has the same proportion of classes (good for imbalanced datasets).\n# \n# GridSearchCV: Finds the best hyperparameters for a model using cross-validation.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.443222Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.443851Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.481825Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.443829Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.481079Z\"}}\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# %% [markdown]\n# Add the dataset to your notebook\n# \n# When you create a new Kaggle Notebook, datasets are not attached automatically.\n# You need to add the Titanic dataset manually:\n# \n# üëâ How to fix:\n# \n# In your Kaggle notebook, look at the right sidebar.\n# \n# Find the section called ‚ÄúAdd Input‚Äù \n# \n# Search for ‚ÄúTitanic - Machine Learning from Disaster‚Äù.\n# \n# Click ‚ÄúAdd Input‚Äù.\n# \n# Then the dataset will appear in /kaggle/input/titanic/.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.482723Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.482999Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.509796Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.482977Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.508997Z\"}}\ntrain_df.head()\n\n# %% [markdown]\n# train_df.head() is a pandas command that shows the first few rows of your DataFrame train_df.\n# \n# train_df: This is your DataFrame (usually loaded from a CSV or Excel file using pd.read_csv() or similar).\n# \n# .head(): By default, it shows the first 5 rows of the DataFrame.\n# \n# You can also specify a number to see more or fewer rows, e.g., train_df.head(10) will show the first 10 rows.\n# \n# Why use it:\n# \n# Quickly inspect your dataset.\n# \n# Check column names, data types, and if the data looks correct.\n# \n# See missing values or unusual entries right away.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.510778Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.511116Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.531741Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.511089Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.530992Z\"}}\ntrain_df.info()\n\n# %% [markdown]\n# train_df.info() is another pandas function that gives you a concise summary of your DataFrame. It‚Äôs extremely useful for understanding the structure of your dataset before any preprocessing or modeling.\n# What it shows:\n# \n# Index range ‚Äì e.g., RangeIndex: 891 entries, 0 to 890\n# Tells you how many rows are in your dataset.\n# \n# Column names and data types ‚Äì e.g.,\n# Column    Non-Null Count  Dtype\n# \n#     Column: Name of each column.\n# \n#     Non-Null Count: How many values in that column are not missing.\n# \n#     Dtype: Data type of the column (e.g., int64, float64, object).\n# \n#     Memory usage ‚Äì How much memory the DataFrame is consuming.\n# What you learn from this output:\n# \n# The Name column has a missing value.\n# \n# Age is numeric (int64), while Name and City are strings (object).\n# \n# There are 4 rows in total.\n# \n# This is usually one of the first commands you run after loading your data, so you can see missing values, data types, and overall structure.    \n#     \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.534597Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.534837Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.564801Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.534819Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.564078Z\"}}\ntrain_df.describe()\n\n# %% [markdown]\n# train_df.describe() is another pandas function that gives you a statistical summary of the numeric columns in your DataFrame\n# | Statistic | Meaning                                         |\n# | --------- | ----------------------------------------------- |\n# | **count** | Number of non-missing values                    |\n# | **mean**  | Average value                                   |\n# | **std**   | Standard deviation (how spread out the data is) |\n# | **min**   | Minimum value                                   |\n# | **25%**   | First quartile (25th percentile)                |\n# | **50%**   | Median (50th percentile)                        |\n# | **75%**   | Third quartile (75th percentile)                |\n# | **max**   | Maximum value                                   |\n# \n# Insights you can get:\n# \n# Average values: e.g., mean age = 28\n# \n# Range: min/max values\n# \n# Spread: standard deviation\n# \n# Quartiles: helps detect outliers\n# \n# Notes:\n# \n# By default, .describe() only includes numeric columns.\n# \n# To include categorical columns, use:\n# \n# train_df.describe(include='all')\n# \n# This will show counts, unique values, top frequent values, and frequency.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.565534Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.565778Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.599949Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.565756Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.599100Z\"}}\ntrain_df.describe(include='all')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.600793Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.601058Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.617876Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.601035Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.617196Z\"}}\ntrain_df.describe(include=['O'])\n\n# %% [markdown]\n#  train_df.describe(include=['O']) is the categorical version of .describe(). Let me explain:\n# \n# What it does\n# \n# 'O' stands for object dtype in pandas, which usually means categorical/text columns.\n# \n# This will give you a summary of all categorical columns instead of numeric ones.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.618754Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.619040Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.629663Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.619022Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.628490Z\"}}\ntrain_df.groupby(['Pclass'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# groupby(['Pclass']): Groups the dataset by the Pclass column (passenger class).\n# \n# ['Survived']: Selects the Survived column to analyze.\n# \n# .mean(): Calculates the average survival rate for each passenger class.\n# \n# as_index=False: Keeps Pclass as a column instead of making it the index.\n# \n# It‚Äôs a quick way to see how class affected survival.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.630700Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.631064Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.648465Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.631045Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.647692Z\"}}\ntrain_df.groupby(['Sex'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# same explanation: \"train_df.groupby(['Pclass'], as_index=False)['Survived'].mean()\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.649240Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.649521Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.667703Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.649495Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.666748Z\"}}\ntrain_df.groupby(['SibSp'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# same explanation: \"train_df.groupby(['Pclass'], as_index=False)['Survived'].mean()\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.668734Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.669037Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.688573Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.669018Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.687722Z\"}}\ntrain_df.groupby(['Parch'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# same explanation: \"train_df.groupby(['Pclass'], as_index=False)['Survived'].mean()\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.689385Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.689697Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.703100Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.689609Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.702225Z\"}}\ntrain_df['Family_Size'] = train_df['SibSp'] + train_df['Parch'] + 1\ntest_df['Family_Size'] = test_df['SibSp'] + test_df['Parch'] + 1\n\n# %% [markdown]\n# SibSp: Number of siblings/spouses aboard.\n# \n# Parch: Number of parents/children aboard.\n# \n# + 1: Includes the passenger themselves.\n# \n# Family_Size: A new feature representing the total number of family members on board.\n# \n# Purpose: Helps capture the effect of traveling alone vs. with family on survival.\n# \n# Example: If a passenger has 1 sibling and 1 parent aboard ‚Üí Family_Size = 1 + 1 + 1 = 3.\n# \n# This is a feature engineering step often used in Titanic datasets.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.704169Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.704611Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.723306Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.704585Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.722521Z\"}}\ntrain_df.groupby(['Family_Size'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# same explanation: \"train_df.groupby(['Pclass'], as_index=False)['Survived'].mean()\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.724225Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.724483Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.740439Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.724457Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.739569Z\"}}\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ntrain_df['Family_Size_Grouped'] = train_df['Family_Size'].map(family_map)\ntest_df['Family_Size_Grouped'] = train_df['Family_Size'].map(family_map)\n\n# %% [markdown]\n# family_map: A dictionary that categorizes family sizes into groups:\n# \n# 1 ‚Üí Alone\n# \n# 2-4 ‚Üí Small\n# \n# 5-6 ‚Üí Medium\n# \n# 7+ ‚Üí Large\n# \n# .map(family_map): Converts each passenger‚Äôs numeric Family_Size into a categorical label.\n# \n# Family_Size_Grouped: New column representing family size category instead of raw numbers.\n# \n# Purpose: Simplifies the feature and helps models capture survival patterns based on family size.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.741287Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.741555Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.759179Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.741536Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.758323Z\"}}\ntrain_df.groupby(['Family_Size_Grouped'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# This line calculates the average survival rate for each family size group.\n# groupby(['Family_Size_Grouped']): Groups passengers by the Family_Size_Grouped column (Alone, Small, Medium, Large).\n# \n# ['Survived']: Selects the Survived column.\n# \n# .mean(): Computes the average survival rate for each group.\n# \n# as_index=False: Keeps Family_Size_Grouped as a column instead of index.\n# \n# Result: Table showing how survival probability changes with family size, e.g.:\n# \n# Family_Size_Grouped\tSurvived\n# Alone\t0.30\n# Small\t0.45\n# Medium\t0.60\n# Large\t0.25\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.760114Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.760400Z\",\"iopub.status.idle\":\"2025-10-18T09:30:24.782587Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.760375Z\",\"shell.execute_reply\":\"2025-10-18T09:30:24.781799Z\"}}\ntrain_df.groupby(['Embarked'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# This line calculates the average survival rate based on the port of embarkation.\n# \n# groupby(['Embarked']): Groups passengers by the Embarked column (port where they boarded: C, Q, S).\n# \n# ['Survived']: Selects the survival column.\n# \n# .mean(): Computes the average survival rate for each embarkation port.\n# \n# as_index=False: Keeps Embarked as a column instead of making it the index.\n# \n# Result: Table showing survival probability by boarding port, e.g.:\n# \n# Embarked\tSurvived\n# C\t0.55\n# Q\t0.38\n# S\t0.34\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:24.783368Z\",\"iopub.execute_input\":\"2025-10-18T09:30:24.783590Z\",\"iopub.status.idle\":\"2025-10-18T09:30:25.396387Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:24.783567Z\",\"shell.execute_reply\":\"2025-10-18T09:30:25.395498Z\"}}\nsns.displot(train_df, x='Age', col='Survived', binwidth=10, height=5)\n\n# %% [markdown]\n# This line creates a distribution plot of Age, separated by survival status using Seaborn:\n# Explanation:\n# \n# train_df: The DataFrame with your data.\n# \n# x='Age': Plots the Age values on the x-axis.\n# \n# col='Survived': Creates separate plots for each survival class (0 = did not survive, 1 = survived).\n# \n# binwidth=10: Sets the width of each histogram bin to 10 years.\n# \n# height=5: Sets the height of each subplot in inches.\n# \n# Result: Two histograms side by side showing age distribution for passengers who survived vs. those who didn‚Äôt.\n# \n# Insight:\n# \n# You can visually see which age groups had higher survival.\n# \n# For example, younger passengers may have higher survival rates (common in Titanic data).\n# \n# It‚Äôs a simple exploratory data analysis (EDA) visualization.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:25.397273Z\",\"iopub.execute_input\":\"2025-10-18T09:30:25.397509Z\",\"iopub.status.idle\":\"2025-10-18T09:30:25.413021Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:25.397490Z\",\"shell.execute_reply\":\"2025-10-18T09:30:25.412336Z\"}}\ntrain_df['Age_Cut'] = pd.qcut(train_df['Age'], 5)\ntest_df['Age_Cut'] = pd.qcut(test_df['Age'], 5)\n\n# %% [markdown]\n# This code is creating a categorical version of the Age column by dividing it into quantiles.\n# pd.qcut(): Cuts a numeric column into equal-sized bins based on quantiles.\n# \n# Here, 5 means the data will be divided into 5 groups (quintiles).\n# \n# train_df['Age_Cut']: New column storing which age bin each passenger belongs to.\n# \n# Purpose: Converts continuous age into categories, which can help some models detect patterns without assuming linearity.\n# \n# Insight: This is useful in feature engineering to capture age-related survival patterns more clearly.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:25.413805Z\",\"iopub.execute_input\":\"2025-10-18T09:30:25.414100Z\",\"iopub.status.idle\":\"2025-10-18T09:30:25.424640Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:25.414075Z\",\"shell.execute_reply\":\"2025-10-18T09:30:25.423900Z\"}}\ntrain_df.groupby(['Age_Cut'], as_index=False)['Survived'].mean()\n\n\n# %% [markdown]\n# This line calculates the average survival rate for each age group (quintile) you created with pd.qcut:\n# groupby(['Age_Cut']): Groups passengers by the age bin (Age_Cut).\n# \n# ['Survived']: Selects the survival column.\n# \n# .mean(): Calculates the average survival rate for each age group.\n# \n# as_index=False: Keeps Age_Cut as a column instead of index.\n# \n# Result: A table showing survival probability for each age group.\n# \n# Insight:\n# \n# Shows which age ranges had higher or lower survival.\n# \n# Helps guide feature engineering (e.g., creating age categories for modeling).\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:25.425399Z\",\"iopub.execute_input\":\"2025-10-18T09:30:25.425681Z\",\"iopub.status.idle\":\"2025-10-18T09:30:25.451566Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:25.425656Z\",\"shell.execute_reply\":\"2025-10-18T09:30:25.450837Z\"}}\ntrain_df.loc[train_df['Age'] <= 19, 'Age'] = 0\ntrain_df.loc[(train_df['Age'] > 19) & (train_df['Age'] <= 25), 'Age'] = 1\ntrain_df.loc[(train_df['Age'] > 25) & (train_df['Age'] <= 31.8), 'Age'] = 2\ntrain_df.loc[(train_df['Age'] > 31.8) & (train_df['Age'] <= 41), 'Age'] = 3\ntrain_df.loc[(train_df['Age'] > 41) & (train_df['Age'] <= 80), 'Age'] = 4\ntrain_df.loc[train_df['Age'] > 80, 'Age'] \n\ntest_df.loc[test_df['Age'] <= 19, 'Age'] = 0\ntest_df.loc[(test_df['Age'] > 19) & (test_df['Age'] <= 25), 'Age'] = 1\ntest_df.loc[(test_df['Age'] > 25) & (test_df['Age'] <= 31.8), 'Age'] = 2\ntest_df.loc[(test_df['Age'] > 31.8) & (test_df['Age'] <= 41), 'Age'] = 3\ntest_df.loc[(test_df['Age'] > 41) & (test_df['Age'] <= 80), 'Age'] = 4\ntest_df.loc[test_df['Age'] > 80, 'Age'] \n\n# %% [markdown]\n# This code is binning the Age column into numeric categories for modeling.\n# For train_df (same logic applies to test_df):\n# Uses loc to select rows based on a condition and assign a new value.\n# \n# Converts continuous Age into discrete age groups (0-4).\n# | Age Range | New Value |\n# | --------- | --------- |\n# | 0‚Äì19      | 0         |\n# | 20‚Äì25     | 1         |\n# | 26‚Äì31.8   | 2         |\n# | 31.8‚Äì41   | 3         |\n# | 41‚Äì80     | 4         |\n# \n# Any ages > 80 remain unchanged in this snippet (could be handled separately).\n# \n# Purpose:\n# \n# Simplifies Age for modeling (many ML models work better with discrete categories).\n# \n# Reduces the effect of outliers.\n# \n# Captures survival patterns by age groups rather than exact values.\n# \n# This is a feature engineering step often done in Titanic datasets to improve model performance.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:25.452361Z\",\"iopub.execute_input\":\"2025-10-18T09:30:25.452591Z\",\"iopub.status.idle\":\"2025-10-18T09:30:25.466032Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:25.452568Z\",\"shell.execute_reply\":\"2025-10-18T09:30:25.465329Z\"}}\ntrain_df.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:25.466811Z\",\"iopub.execute_input\":\"2025-10-18T09:30:25.467074Z\",\"iopub.status.idle\":\"2025-10-18T09:30:25.944421Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:25.467057Z\",\"shell.execute_reply\":\"2025-10-18T09:30:25.943702Z\"}}\nsns.displot(train_df, x='Fare', col='Survived', binwidth=80, height=5)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:25.950010Z\",\"iopub.execute_input\":\"2025-10-18T09:30:25.950287Z\",\"iopub.status.idle\":\"2025-10-18T09:30:25.962000Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:25.950268Z\",\"shell.execute_reply\":\"2025-10-18T09:30:25.961113Z\"}}\ntrain_df['Fare_Cut'] = pd.qcut(train_df['Fare'], 5)\ntest_df['Fare_Cut'] = pd.qcut(test_df['Fare'], 5)\n\n# %% [markdown]\n# This code divides the Fare column into 5 quantile-based bins:\n# \n# pd.qcut(train_df['Fare'], 5) ‚Üí splits fares into 5 groups with roughly equal number of passengers.\n# \n# Fare_Cut ‚Üí new column storing the fare category.\n# \n# Purpose: Converts continuous fare values into categorical bins for easier modeling.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:25.962773Z\",\"iopub.execute_input\":\"2025-10-18T09:30:25.963086Z\",\"iopub.status.idle\":\"2025-10-18T09:30:25.983701Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:25.963059Z\",\"shell.execute_reply\":\"2025-10-18T09:30:25.982976Z\"}}\ntrain_df.groupby(['Fare_Cut'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# This line calculates the average survival rate for each fare category:\n# \n# groupby(['Fare_Cut']) ‚Üí groups passengers by fare bins.\n# \n# ['Survived'].mean() ‚Üí computes the survival probability for each group.\n# \n# Purpose: Shows how survival varies with ticket price.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:25.984690Z\",\"iopub.execute_input\":\"2025-10-18T09:30:25.985019Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.006975Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:25.984993Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.006203Z\"}}\ntrain_df.loc[train_df['Fare'] <= 7.854, 'Fare'] = 0\ntrain_df.loc[(train_df['Fare'] > 7.854) & (train_df['Fare'] <= 10.5), 'Fare'] = 1\ntrain_df.loc[(train_df['Fare'] > 10.5) & (train_df['Fare'] <= 21.679), 'Fare'] = 2\ntrain_df.loc[(train_df['Fare'] > 21.679) & (train_df['Fare'] <= 39.688), 'Fare'] = 3\ntrain_df.loc[(train_df['Fare'] > 39.688) & (train_df['Fare'] <= 512.329), 'Fare'] = 4\ntrain_df.loc[train_df['Fare'] > 512.329, 'Fare'] \n\ntest_df.loc[test_df['Fare'] <= 7.854, 'Fare'] = 0\ntest_df.loc[(test_df['Fare'] > 7.854) & (test_df['Fare'] <= 10.5), 'Fare'] = 1\ntest_df.loc[(test_df['Fare'] > 10.5) & (test_df['Fare'] <= 21.679), 'Fare'] = 2\ntest_df.loc[(test_df['Fare'] > 21.679) & (test_df['Fare'] <= 39.688), 'Fare'] = 3\ntest_df.loc[(test_df['Fare'] > 39.688) & (test_df['Fare'] <= 512.329), 'Fare'] = 4\ntest_df.loc[test_df['Fare'] > 512.329, 'Fare'] \n\n# %% [markdown]\n# This code bins the Fare column into discrete categories (0‚Äì4) for modeling:\n# \n# Uses loc to assign a number based on fare ranges.\n# \n# Converts continuous fare values into ordinal categories.\n# \n# Purpose: Simplifies Fare, reduces outlier impact, and helps models detect survival patterns by fare groups.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.007979Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.008271Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.025613Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.008246Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.024928Z\"}}\ntrain_df['Name']\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.026509Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.026789Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.045655Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.026765Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.044707Z\"}}\ntrain_df['Title'] = train_df['Name'].str.split(pat= \",\", expand=True)[1].str.split(pat= \".\", expand=True)[0].apply(lambda x: x.strip())\ntest_df['Title'] = test_df['Name'].str.split(pat= \",\", expand=True)[1].str.split(pat= \".\", expand=True)[0].apply(lambda x: x.strip())\n\n# %% [markdown]\n# This code extracts the title (e.g., Mr, Mrs, Miss) from the Name column:\n# \n# Splits Name by , ‚Üí takes the part after the comma.\n# \n# Splits that part by . ‚Üí takes the title before the period.\n# \n# .strip() ‚Üí removes extra spaces.\n# \n# Title column stores these extracted titles for feature use.\n# \n# Purpose: Captures social status/gender info, which can affect survival.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.046566Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.046916Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.065745Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.046888Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.064864Z\"}}\ntrain_df.groupby(['Title'], as_index=False)['Survived'].mean()\n\n# %% [markdown]\n# #military - Capt, Col, Major\n# #noble - Jonkheer, the Countess, Don, Lady, Sir\n# #unmaried Female - Mlle, Ms, Mme\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.066740Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.067136Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.082531Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.067110Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.081801Z\"}}\ntrain_df['Title'] = train_df['Title'].replace({\n    'Capt': 'Military',\n    'Col': 'Military',\n    'Major': 'Military',\n    'Jonkheer': 'Noble',\n    'the Countess': 'Noble',\n    'Don': 'Noble',\n    'Lady': 'Noble',\n    'Sir': 'Noble',\n    'Mlle': 'Noble',\n    'Ms': 'Noble',\n    'Mme': 'Noble'    \n})\n\ntest_df['Title'] = test_df['Title'].replace({\n    'Capt': 'Military',\n    'Col': 'Military',\n    'Major': 'Military',\n    'Jonkheer': 'Noble',\n    'the Countess': 'Noble',\n    'Don': 'Noble',\n    'Lady': 'Noble',\n    'Sir': 'Noble',\n    'Mlle': 'Noble',\n    'Ms': 'Noble',\n    'Mme': 'Noble'    \n})\n\n# %% [markdown]\n# This code groups rare titles into broader categories:\n# \n# Military: Capt, Col, Major ‚Üí \"Military\"\n# \n# Noble: Jonkheer, the Countess, Don, Lady, Sir ‚Üí \"Noble\"\n# \n# Unmarried female titles (Mlle, Ms, Mme) ‚Üí \"Noble\" (for simplicity)\n# \n# Purpose: Reduces the number of unique title categories to make modeling easier.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.083392Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.083687Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.100965Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.083662Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.100100Z\"}}\ntrain_df.groupby(['Title'], as_index=False)['Survived'].agg(['count', 'mean'])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.101716Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.102296Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.116390Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.102269Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.115575Z\"}}\ntrain_df['Name_Length'] = train_df['Name'].apply(lambda x: len(x))\ntest_df['Name_Length'] = test_df['Name'].apply(lambda x: len(x))\n\n# %% [markdown]\n# This code creates a new feature Name_Length showing how long each passenger‚Äôs name is:\n# \n# len(x) ‚Üí counts characters in each name.\n# \n# Purpose: Captures possible social/class patterns (longer names may indicate higher status).\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.117297Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.117537Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.388873Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.117520Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.388053Z\"}}\ng = sns.kdeplot(train_df['Name_Length'][(train_df['Survived']==0) & (train_df['Name_Length'].notnull())], color='Red', fill=True)\ng = sns.kdeplot(train_df['Name_Length'][(train_df['Survived']==1) & (train_df['Name_Length'].notnull())], ax=g, color='Blue', fill=True)\ng.set_xlabel('Name_Length')\ng.set_ylabel('Frequency')\ng = g.legend(['Not Survived', 'Survived'])\n\n# %% [markdown]\n# This code plots KDE (density) curves of Name_Length for survival outcomes:\n# \n# Red curve: passengers who didn‚Äôt survive.\n# \n# Blue curve: passengers who survived.\n# \n# Shows how name length distribution differs by survival status.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.389666Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.389890Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.401323Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.389872Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.400443Z\"}}\ntrain_df['Name_LengthGB'] = pd.qcut(train_df['Name_Length'], 3)\ntest_df['Name_LengthGB'] = pd.qcut(test_df['Name_Length'], 3)\n\n# %% [markdown]\n# This code divides Name_Length into 3 quantile-based groups (small, medium, long names) using pd.qcut().\n# \n# Purpose: Turns continuous name length into categorical bins for easier pattern detection in modeling.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.402048Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.402304Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.423186Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.402285Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.422432Z\"}}\ntrain_df.groupby(['Name_LengthGB'], as_index=False)['Survived'].mean()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.424011Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.424259Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.446879Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.424242Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.446054Z\"}}\ntrain_df.loc[train_df['Name_Length'] <= 22, 'Name_Size'] = 0\ntrain_df.loc[(train_df['Name_Length'] > 22) & (train_df['Name_Length'] <= 28), 'Name_Size'] = 1\ntrain_df.loc[(train_df['Name_Length'] > 28) & (train_df['Name_Length'] <= 82), 'Name_Size'] = 2\ntrain_df.loc[train_df['Name_Length'] > 82, 'Name_Size'] \n\ntest_df.loc[test_df['Name_Length'] <= 22, 'Name_Size'] = 0\ntest_df.loc[(test_df['Name_Length'] > 22) & (test_df['Name_Length'] <= 28), 'Name_Size'] = 1\ntest_df.loc[(test_df['Name_Length'] > 28) & (test_df['Name_Length'] <= 82), 'Name_Size'] = 2\ntest_df.loc[test_df['Name_Length'] > 82, 'Name_Size'] \n\n# %% [markdown]\n# ChatGPT said:\n# \n# This code bins Name_Length into 3 categories:\n# \n# Length Range\tName_Size\n# ‚â§ 22\t0\n# 23‚Äì28\t1\n# 29‚Äì82\t2\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.447694Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.447997Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.465211Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.447971Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.464389Z\"}}\ntrain_df.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.466301Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.466536Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.482382Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.466518Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.481514Z\"}}\ntrain_df['Ticket']\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.483272Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.483560Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.680542Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.483541Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.679705Z\"}}\ntrain_df['TicketNumber'] = train_df['Ticket'].apply(lambda x: pd.Series({'Ticket': x.split()[-1]}))\ntest_df['TicketNumber'] = test_df['Ticket'].apply(lambda x: pd.Series({'Ticket': x.split()[-1]}))\n\n# %% [markdown]\n# This code extracts the last part of each ticket string (usually the numeric part) and stores it in a new column TicketNumber.\n# \n# Purpose: Isolates ticket numbers from mixed text‚Äìnumber ticket codes for analysis or modeling.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.681446Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.681752Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.694632Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.681725Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.693917Z\"}}\ntrain_df.groupby(['TicketNumber'], as_index=False)['Survived'].agg(['count', 'mean']).sort_values('count', ascending=False)\n\n# %% [markdown]\n# This line performs a grouped summary of survival by ticket number:\n# \n# \n# groupby(['TicketNumber']) ‚Üí Groups passengers who share the same ticket.\n# \n# ['Survived'] ‚Üí Focuses only on the Survived column.\n# \n# .agg(['count', 'mean']) ‚Üí\n# \n# count: number of passengers with that ticket.\n# \n# mean: average survival rate of those passengers.\n# \n# .sort_values('count', ascending=False) ‚Üí Sorts so tickets with the most passengers appear first.\n# \n# Purpose:\n# \n# To see how many people shared each ticket and how their group survival rates compare ‚Äî useful for identifying family or group survival trends.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.695504Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.695818Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.706383Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.695791Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.705608Z\"}}\ntrain_df.groupby('TicketNumber')['TicketNumber'].transform('count')\n\n# %% [markdown]\n# groupby('TicketNumber') ‚Üí Groups rows with the same ticket number.\n# \n# ['TicketNumber'] ‚Üí Focuses on that column.\n# \n# .transform('count') ‚Üí Counts how many times each ticket appears,\n# but returns the count for every row (keeping the same DataFrame shape).\n# \n# Result:\n# \n# Each passenger gets a number showing how many people shared their ticket.\n# \n# Purpose:\n# Adds a feature showing group size by ticket, useful for finding families or travel groups.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.707122Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.707382Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.723212Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.707364Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.722429Z\"}}\ntrain_df['TicketNumberCounts'] = train_df.groupby('TicketNumber')['TicketNumber'].transform('count')\ntest_df['TicketNumberCounts'] = test_df.groupby('TicketNumber')['TicketNumber'].transform('count')\n\n# %% [markdown]\n# This code counts how many passengers share the same ticket number and stores that count in a new column called TicketNumberCounts for both train_df and test_df.\n# ‚Üí It helps identify group or family size based on shared tickets.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.724218Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.724609Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.735012Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.724582Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.734294Z\"}}\ntrain_df.groupby(['TicketNumberCounts'], as_index=False)['Survived'].agg(['count', 'mean']).sort_values('count', ascending=False)\n\n# %% [markdown]\n# This code groups passengers by how many people shared their ticket (TicketNumberCounts), then:\n# \n# count ‚Üí shows how many passengers are in each group size,\n# \n# mean ‚Üí shows the average survival rate for that group size,\n# \n# sort_values('count', ascending=False) ‚Üí sorts to show the most common group sizes first.\n# \n# ‚úÖ Goal: See how survival chances change with ticket group size.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.735925Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.736820Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.754105Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.736792Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.753279Z\"}}\ntrain_df['Ticket']\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.755075Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.755381Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.773436Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.755357Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.772726Z\"}}\ntrain_df['Ticket'].str.split(pat=\" \", expand=True)\n\n# %% [markdown]\n# This code splits the values in the Ticket column by spaces (\" \") and expands the result into separate columns.\n# ‚úÖ Goal: Separate ticket prefixes (like letters) from ticket numbers for analysis.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.774385Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.774648Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.795855Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.774629Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.794989Z\"}}\ntrain_df['TicketLocation'] = np.where(train_df['Ticket'].str.split(pat=\" \", expand=True)[1].notna(), train_df['Ticket'].str.split(pat=\" \", expand=True)[0].apply(lambda x: x.strip()), 'Blank')\ntest_df['TicketLocation'] = np.where(test_df['Ticket'].str.split(pat=\" \", expand=True)[1].notna(), test_df['Ticket'].str.split(pat=\" \", expand=True)[0].apply(lambda x: x.strip()), 'Blank')\n\n# %% [markdown]\n# This code extracts the **prefix (letters)** from the `Ticket` column and stores it in a new column called **`TicketLocation`**.\n# \n# * If the ticket has a prefix (like `\"A/5 21171\"` ‚Üí `\"A/5\"`), it keeps that prefix.\n# * If there‚Äôs **no prefix**, it fills it with **`'Blank'`**.\n# \n# ‚úÖ **Goal:** Create a feature showing the **ticket‚Äôs origin or class code**, which may relate to passenger groups or cabins.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.796709Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.797617Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.813877Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.797589Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.813199Z\"}}\ntrain_df['TicketLocation'].value_counts()\n\n# %% [markdown]\n# This line counts how many times each **ticket prefix (`TicketLocation`)** appears in the dataset.\n# \n# ‚úÖ **Result:** Shows the **frequency of each ticket code** (like ‚ÄúPC‚Äù, ‚ÄúA/5‚Äù, ‚ÄúSTON/O‚Äù, or ‚ÄúBlank‚Äù).\n# üëâ Helps identify which ticket prefixes were most common among passengers.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.814747Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.815054Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.834170Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.815029Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.833388Z\"}}\ntrain_df['TicketLocation'] = train_df['TicketLocation'].replace({\n    'SOTON/O.Q.':'SOTON/OQ',\n    'C.A.':'CA',\n    'CA.':'CA',\n    'SC/PARIS':'SC/Paris',\n    'S.C./PARIS':'SC/Paris',\n    'A/4.':'A/4',\n    'A/5.':'A/5',\n    'A.5.':'A/5',\n    'A./5.':'A/5',\n    'W./C.':'W/C',    \n})\n\ntest_df['TicketLocation'] = test_df['TicketLocation'].replace({\n    'SOTON/O.Q.':'SOTON/OQ',\n    'C.A.':'CA',\n    'CA.':'CA',\n    'SC/PARIS':'SC/Paris',\n    'S.C./PARIS':'SC/Paris',\n    'A/4.':'A/4',\n    'A/5.':'A/5',\n    'A.5.':'A/5',\n    'A./5.':'A/5',\n    'W./C.':'W/C',    \n})\n\n# %% [markdown]\n# This code **cleans and standardizes** ticket prefixes in the `TicketLocation` column by replacing different spellings or formats with a single consistent version (e.g., `'A.5.'` ‚Üí `'A/5'`).\n# \n# ‚úÖ **Goal:** fix inconsistent ticket codes so similar prefixes are treated as the same category.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.834889Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.835192Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.857338Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.835169Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.856330Z\"}}\ntrain_df.groupby(['TicketLocation'], as_index=False)['Survived'].agg(['count', 'mean'])\n\n# %% [markdown]\n# This code groups passengers by **`TicketLocation`** (ticket prefix) and calculates:\n# \n# * **`count`** ‚Üí how many passengers had that ticket prefix,\n# * **`mean`** ‚Üí the average survival rate for that prefix.\n# \n# ‚úÖ **Goal:** see which ticket prefixes (locations or agencies) had higher or lower survival rates.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.858272Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.858582Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.876452Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.858555Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.875674Z\"}}\ntrain_df['Cabin'] = train_df['Cabin'].fillna('U')\ntrain_df['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'x' for i in train_df['Cabin']])\n\ntest_df['Cabin'] = test_df['Cabin'].fillna('U')\ntest_df['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'x' for i in test_df['Cabin']])\n\n# %% [markdown]\n# This code handles the **`Cabin`** column:\n# \n# * Fills missing cabin values with `'U'` (unknown).\n# * Keeps **only the first letter** of each cabin (e.g., `\"C85\"` ‚Üí `\"C\"`).\n# \n# ‚úÖ **Goal:** simplify cabin info to deck level and handle missing data.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.877356Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.877619Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.899801Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.877593Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.899115Z\"}}\ntrain_df.groupby(['Cabin'], as_index=False)['Survived'].agg(['count', 'mean'])\n\n\n# %% [markdown]\n# This code groups passengers by **cabin letter** and calculates:\n# \n# * **`count`** ‚Üí how many passengers were in each cabin group,\n# * **`mean`** ‚Üí the average survival rate in that cabin.\n# \n# ‚úÖ **Goal:** see which **deck levels** had more passengers and higher survival chances.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.900606Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.900828Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.912789Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.900811Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.912005Z\"}}\ntrain_df['Cabin_Assigned'] = train_df['Cabin'].apply(lambda x: 0 if x in ['U'] else 1)\ntest_df['Cabin_Assigned'] = test_df['Cabin'].apply(lambda x: 0 if x in ['U'] else 1)\n\n# %% [markdown]\n# This code creates a new feature **`Cabin_Assigned`**:\n# \n# * **0** ‚Üí cabin unknown (`'U'`)\n# * **1** ‚Üí cabin assigned (any other letter)\n# \n# ‚úÖ **Purpose:** A simple binary feature showing whether a passenger had a cabin assigned or not.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.913607Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.913886Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.934374Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.913851Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.933702Z\"}}\ntrain_df.groupby(['Cabin_Assigned'], as_index=False)['Survived'].agg(['count', 'mean'])\n\n# %% [markdown]\n# This code groups passengers by **`Cabin_Assigned`** (whether they had a cabin or not) and calculates:\n# \n# * **`count`** ‚Üí number of passengers in each group,\n# * **`mean`** ‚Üí average survival rate for that group.\n# \n# ‚úÖ **Goal:** see if having an assigned cabin affected survival chances.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.935159Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.935547Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.963994Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.935522Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.963242Z\"}}\ntrain_df.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.964798Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.965058Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.969869Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.965039Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.969219Z\"}}\ntrain_df.shape\n\n# %% [markdown]\n# This command:\n# \n# ```python\n# train_df.shape\n# ```\n# \n# returns the **dimensions of the DataFrame** as a tuple `(rows, columns)`.\n# \n# * First value ‚Üí number of rows (passengers)\n# * Second value ‚Üí number of columns (features)\n# \n# ‚úÖ **Purpose:** quickly check the size of your dataset.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.970587Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.970840Z\",\"iopub.status.idle\":\"2025-10-18T09:30:26.986596Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.970822Z\",\"shell.execute_reply\":\"2025-10-18T09:30:26.985912Z\"}}\ntest_df.shape\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:26.987472Z\",\"iopub.execute_input\":\"2025-10-18T09:30:26.987746Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.014259Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:26.987726Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.013339Z\"}}\ntrain_df.info()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.015125Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.015378Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.031636Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.015359Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.030783Z\"}}\ntrain_df.columns\n\n# %% [markdown]\n# \n# returns a **list of all column names** in the DataFrame.\n# \n# ‚úÖ **Purpose:** see which features are available for analysis or modeling.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.032856Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.033152Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.059578Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.033125Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.058768Z\"}}\ntest_df.info()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.060440Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.060670Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.077187Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.060652Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.076333Z\"}}\ntrain_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\ntest_df['Age'].fillna(test_df['Age'].mean(), inplace=True)\ntest_df['Fare'].fillna(test_df['Fare'].mean(), inplace=True)\n\n# %% [markdown]\n# This code **fills missing values** in numeric columns:\n# \n# * `Age` ‚Üí replaces NaNs with the **mean age** of the dataset.\n# * `Fare` (in `test_df`) ‚Üí replaces NaNs with the **mean fare**.\n# * **`inplace=True`** ‚Üí updates the DataFrame directly.\n# \n# ‚úÖ **Purpose:** handle missing numeric data before modeling.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.078053Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.078316Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.092813Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.078295Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.091967Z\"}}\nohe = OneHotEncoder(sparse_output=False)\node = OrdinalEncoder\nSI = SimpleImputer(strategy='most_frequent')\n\n# %% [markdown]\n# This code defines **preprocessing tools** from scikit-learn:\n# \n# * **`OneHotEncoder(sparse_output=False)`** ‚Üí converts categorical features into **binary/dummy columns** (0/1) for each category.\n# * **`OrdinalEncoder`** ‚Üí converts categorical values into **integer labels** (0, 1, 2, ‚Ä¶).\n# * **`SimpleImputer(strategy='most_frequent')`** ‚Üí fills missing values using the **most frequent value** in the column.\n# \n# ‚úÖ **Purpose:** prepare data for machine learning models.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.093739Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.093995Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.107365Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.093973Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.106433Z\"}}\node_cols = ['Family_Size_Grouped']\nohe_cols = ['Sex', 'Embarked']\n\n# %% [markdown]\n# This code defines which columns to encode:\n# \n# * **`ode_cols = ['Family_Size_Grouped']`** ‚Üí will use **Ordinal Encoding** (map categories to integers).\n# * **`ohe_cols = ['Sex', 'Embarked']`** ‚Üí will use **One-Hot Encoding** (create separate binary columns for each category).\n# \n# ‚úÖ **Purpose:** specify how categorical features should be transformed before modeling.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.108271Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.108801Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.835692Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.108779Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.834731Z\"}}\ncorrelation_matrix = train_df.corr(numeric_only=True)\n\n# Create a heatmap using Seaborn\nplt.figure(figsize=(8, 6))  # Adjust the figure size as needed\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n\n# %% [markdown]\n# This code **visualizes correlations between numeric features** in the dataset:\n# \n# 1. **`train_df.corr(numeric_only=True)`** ‚Üí computes the **Pearson correlation matrix** for numeric columns.\n# 2. **`plt.figure(figsize=(8,6))`** ‚Üí sets the plot size.\n# 3. **`sns.heatmap(..., annot=True, cmap='coolwarm', fmt=\".2f\")`** ‚Üí draws a **heatmap** with values annotated, using a red-blue color map.\n# \n# ‚úÖ **Purpose:** quickly see which numeric features are **positively or negatively correlated** with each other or with `Survived`.\n# \n\n# %% [markdown]\n# #NEW Drop Sibsp, Parch, TicketNumberCounts\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.836594Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.836837Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.844013Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.836818Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.843159Z\"}}\n#new\nX = train_df.drop(['Survived', 'SibSp', 'Parch'], axis=1)\ny = train_df['Survived']\nX_test = test_df.drop(['Age_Cut', 'Fare_Cut', 'SibSp', 'Parch'], axis=1)\n\n# %% [markdown]\n# This code **prepares the features (`X`) and target (`y`) for modeling**:\n# \n# * **`X`** ‚Üí training features, drops:\n# \n#   * `Survived` (target)\n#   * `SibSp` and `Parch` (redundant because `Family_Size` is already used)\n# * **`y`** ‚Üí target variable (`Survived`)\n# * **`X_test`** ‚Üí test features, drops extra engineered columns (`Age_Cut`, `Fare_Cut`) and redundant family columns (`SibSp`, `Parch`)\n# \n# ‚úÖ **Purpose:** create clean feature sets for training and prediction.\n# \n\n# %% [markdown]\n# #OLD\n# #X = train_df.drop(['Survived'], axis=1)\n# #y = train_df['Survived']\n# #X_test = test_df.drop(['Age_Cut', 'Fare_Cut'], axis=1)\n# This commented-out code shows the **older version of feature selection**:\n# \n# * **`X`** ‚Üí used all columns except `Survived` (didn‚Äôt drop `SibSp` or `Parch`)\n# * **`X_test`** ‚Üí dropped only `Age_Cut` and `Fare_Cut`\n# \n# ‚úÖ **Purpose:** keeps a record of the previous approach before deciding to drop redundant columns for modeling.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.844904Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.845567Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.862791Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.845545Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.861991Z\"}}\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify = y, random_state=21)\n\n# %% [markdown]\n# This code **splits the training data into training and validation sets**:\n# \n# * **`test_size=0.2`** ‚Üí 20% of data goes to validation, 80% for training.\n# * **`stratify=y`** ‚Üí preserves the same proportion of `Survived` classes in both sets.\n# * **`random_state=21`** ‚Üí ensures reproducible splits.\n# \n# ‚úÖ **Purpose:** evaluate models on unseen data without touching the test set.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.863698Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.864014Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.875304Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.863979Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.874407Z\"}}\nordinal_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\n# %% [markdown]\n# This code defines a **preprocessing pipeline for ordinal (integer) encoding**:\n# \n# ### **Steps:**\n# \n# 1. **`impute`** ‚Üí fills missing values with the **most frequent category**.\n# 2. **`ord`** ‚Üí converts categorical values into integers; unknown categories are set to `-1`.\n# \n# ‚úÖ **Purpose:** prepare ordinal categorical features for machine learning models in a clean, automated way.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.876266Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.876579Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.890525Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.876552Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.889703Z\"}}\nohe_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(handle_unknown = 'ignore', sparse_output=False))\n])\n\n# %% [markdown]\n# This code defines a **preprocessing pipeline for one-hot encoding**:\n# ### **Steps:**\n# \n# 1. **`impute`** ‚Üí fills missing values with the **most frequent category**.\n# 2. **`one-hot`** ‚Üí converts categorical values into **binary/dummy columns**; unknown categories are ignored.\n# \n# ‚úÖ **Purpose:** cleanly prepare nominal categorical features for machine learning models.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.891387Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.891762Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.906741Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.891737Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.905836Z\"}}\ncol_trans = ColumnTransformer(transformers=[\n    ('impute', SI, ['Age']),\n    ('ord_pipeline', ordinal_pipeline, ode_cols),\n    ('ohe_pipeline', ohe_pipeline, ohe_cols),\n   # ('passthrough', 'passthrough', ['Pclass', 'TicketNumberCounts', 'Cabin_Assigned', 'Name_Size', 'Age', 'Fare'])\n     ('passthrough', 'passthrough', ['Pclass', 'Cabin_Assigned', 'Name_Size', 'Age', 'Fare', 'TicketNumberCounts'])\n    ],\n    remainder='drop',\n    n_jobs=-1)\n\n# %% [markdown]\n# This code defines a **`ColumnTransformer`** to preprocess different types of features simultaneously:\n# \n# 1. **`SI`** ‚Üí fills missing `Age`.\n# 2. **`ordinal_pipeline`** ‚Üí ordinal encoding for columns in `ode_cols` (like `Family_Size_Grouped`).\n# 3. **`ohe_pipeline`** ‚Üí one-hot encoding for columns in `ohe_cols` (like `Sex`, `Embarked`).\n# 4. **`passthrough`** ‚Üí keeps numeric columns unchanged.\n# 5. **`remainder='drop'`** ‚Üí drops any columns not specified.\n# 6. **`n_jobs=-1`** ‚Üí uses all CPU cores for parallel processing.\n# \n# ‚úÖ **Purpose:** apply different preprocessing steps to different columns in a single, unified pipeline.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.907683Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.907905Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.921969Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.907886Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.921179Z\"}}\nrfc = RandomForestClassifier()\n\n\n# %% [markdown]\n# This line creates a **Random Forest classifier**:\n# \n# * **RandomForestClassifier** ‚Üí an ensemble model that builds multiple decision trees and averages their predictions.\n# * Using default parameters here.\n# \n# ‚úÖ **Purpose:** ready to train a model to predict `Survived` based on features.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.922780Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.923069Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.938384Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.923045Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.937671Z\"}}\nparam_grid = {\n    'n_estimators': [150, 200, 300, 500],\n    'min_samples_split': [5, 10, 15],\n    'max_depth': [10, 13, 15, 17, 20],\n    'min_samples_leaf': [2, 4, 5, 6],\n    'criterion': ['gini', 'entropy'],\n}\n\n# %% [markdown]\n# This code defines a **grid of hyperparameters** for tuning a Random Forest model:\n# \n# * **`n_estimators`** ‚Üí number of trees in the forest.\n# * **`min_samples_split`** ‚Üí minimum samples required to split a node.\n# * **`max_depth`** ‚Üí maximum depth of each tree.\n# * **`min_samples_leaf`** ‚Üí minimum samples required at a leaf node.\n# * **`criterion`** ‚Üí function to measure split quality (`gini` or `entropy`).\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the best combination of hyperparameters for the Random Forest.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.939512Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.939791Z\",\"iopub.status.idle\":\"2025-10-18T09:30:27.956366Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.939772Z\",\"shell.execute_reply\":\"2025-10-18T09:30:27.955512Z\"}}\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up a **Grid Search with cross-validation** to tune the Random Forest:\n# \n# * **`estimator=rfc`** ‚Üí the Random Forest model to tune.\n# * **`param_grid=param_grid`** ‚Üí all combinations of hyperparameters to try.\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold cross-validation that preserves the proportion of classes (`Survived`).\n# \n# ‚úÖ **Purpose:** automatically find the **best hyperparameters** while validating the model on multiple splits.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:30:27.957261Z\",\"iopub.execute_input\":\"2025-10-18T09:30:27.957520Z\",\"iopub.status.idle\":\"2025-10-18T09:47:22.911145Z\",\"shell.execute_reply.started\":\"2025-10-18T09:30:27.957496Z\",\"shell.execute_reply\":\"2025-10-18T09:47:22.910208Z\"}}\npipefinalrfc = make_pipeline(col_trans, CV_rfc)\npipefinalrfc.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline with preprocessing and model tuning**:\n# \n# 1. **`col_trans`** ‚Üí applies all preprocessing steps (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_rfc`** ‚Üí runs GridSearchCV on the Random Forest to find the best hyperparameters using 5-fold cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensure that **preprocessing and model training happen together**, so the model is ready to make predictions on both training and test data.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:22.912002Z\",\"iopub.execute_input\":\"2025-10-18T09:47:22.912356Z\",\"iopub.status.idle\":\"2025-10-18T09:47:22.916697Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:22.912335Z\",\"shell.execute_reply\":\"2025-10-18T09:47:22.915980Z\"}}\nprint(CV_rfc.best_params_)\nprint(CV_rfc.best_score_)\n\n# %% [markdown]\n# This code **displays the results of the Grid Search** for the Random Forest:\n# \n# * **`best_params_`** ‚Üí shows the **hyperparameter combination** that gave the highest cross-validation score.\n# * **`best_score_`** ‚Üí shows the **best average cross-validation accuracy** achieved with those parameters.\n# \n# ‚úÖ **Purpose:** find and review the **optimal model configuration** and its estimated performance.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:22.917631Z\",\"iopub.execute_input\":\"2025-10-18T09:47:22.917874Z\",\"iopub.status.idle\":\"2025-10-18T09:47:22.934759Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:22.917849Z\",\"shell.execute_reply\":\"2025-10-18T09:47:22.933884Z\"}}\ndtc = DecisionTreeClassifier()\n\n# %% [markdown]\n# This line creates a **Decision Tree classifier**:\n# \n# * **DecisionTreeClassifier** ‚Üí a single tree-based model that splits data based on feature values to predict the target.\n# * Using **default parameters** here.\n# \n# ‚úÖ **Purpose:** ready to train a Decision Tree model for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:22.935673Z\",\"iopub.execute_input\":\"2025-10-18T09:47:22.935965Z\",\"iopub.status.idle\":\"2025-10-18T09:47:22.950303Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:22.935920Z\",\"shell.execute_reply\":\"2025-10-18T09:47:22.949417Z\"}}\nparam_grid = {\n    'min_samples_split': [5, 10, 15],\n    'max_depth': [10, 20, 30],\n    'min_samples_leaf': [1, 2, 4],\n    'criterion': ['gini', 'entropy'],\n}\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning a Decision Tree model:\n# \n# * **`min_samples_split`** ‚Üí minimum samples required to split a node.\n# * **`max_depth`** ‚Üí maximum depth of the tree.\n# * **`min_samples_leaf`** ‚Üí minimum samples required at a leaf node.\n# * **`criterion`** ‚Üí function to measure split quality (`gini` or `entropy`).\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the best Decision Tree configuration.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:22.951221Z\",\"iopub.execute_input\":\"2025-10-18T09:47:22.951469Z\",\"iopub.status.idle\":\"2025-10-18T09:47:22.966696Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:22.951449Z\",\"shell.execute_reply\":\"2025-10-18T09:47:22.965798Z\"}}\nCV_dtc = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for the Decision Tree:\n# \n# * **`param_grid=param_grid`** ‚Üí all combinations of hyperparameters to test.\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold cross-validation keeping the proportion of `Survived` classes in each fold.\n# \n# ‚úÖ **This code **creates and trains a full pipeline for the Decision Tree**:\n# \n# 1. **`col_trans`** ‚Üí applies all preprocessing steps (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_dtc`** ‚Üí runs GridSearchCV to find the **best Decision Tree hyperparameters** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and model training happen together**, producing a Decision Tree model ready for prediction.\n# Purpose:** automatically find the **best hyperparameters** for the Decision Tree while validating its performance.\n# ****\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:22.967493Z\",\"iopub.execute_input\":\"2025-10-18T09:47:22.967707Z\",\"iopub.status.idle\":\"2025-10-18T09:47:25.417703Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:22.967691Z\",\"shell.execute_reply\":\"2025-10-18T09:47:25.416903Z\"}}\npipefinaldtc = make_pipeline(col_trans, CV_dtc)\npipefinaldtc.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for the Decision Tree**:\n# \n# 1. **`col_trans`** ‚Üí applies all preprocessing steps (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_dtc`** ‚Üí runs GridSearchCV to find the **best Decision Tree hyperparameters** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and model training happen together**, producing a Decision Tree model ready for prediction.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:25.426714Z\",\"iopub.execute_input\":\"2025-10-18T09:47:25.427142Z\",\"iopub.status.idle\":\"2025-10-18T09:47:25.431372Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:25.427117Z\",\"shell.execute_reply\":\"2025-10-18T09:47:25.430702Z\"}}\nprint(CV_dtc.best_params_)\nprint(CV_dtc.best_score_)\n\n# %% [markdown]\n# This code **displays the results of the Decision Tree Grid Search**:\n# \n# * **`best_params_`** ‚Üí shows the **hyperparameter combination** that achieved the highest cross-validation score.\n# * **`best_score_`** ‚Üí shows the **best average cross-validation accuracy** with those parameters.\n# \n# ‚úÖ **Purpose:** review the **optimal Decision Tree configuration** and its estimated performance.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:25.432163Z\",\"iopub.execute_input\":\"2025-10-18T09:47:25.432500Z\",\"iopub.status.idle\":\"2025-10-18T09:47:25.446212Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:25.432473Z\",\"shell.execute_reply\":\"2025-10-18T09:47:25.445423Z\"}}\nknn = KNeighborsClassifier()\n\n# %% [markdown]\n# This line creates a **K-Nearest Neighbors (KNN) classifier**:\n# \n# * **KNeighborsClassifier** ‚Üí a model that predicts a class based on the **majority class of the k nearest neighbors** in the feature space.\n# * Using **default parameters** here.\n# \n# ‚úÖ **Purpose:** ready to train a KNN model for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:25.447019Z\",\"iopub.execute_input\":\"2025-10-18T09:47:25.447296Z\",\"iopub.status.idle\":\"2025-10-18T09:47:25.463179Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:25.447272Z\",\"shell.execute_reply\":\"2025-10-18T09:47:25.462317Z\"}}\nparam_grid = {\n    'n_neighbors': [3, 5, 7, 9, 11],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'p': [1,2],\n}\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning a K-Nearest Neighbors (KNN) model:\n# \n# * **`n_neighbors`** ‚Üí number of neighbors to consider for voting.\n# * **`weights`** ‚Üí how neighbors are weighted (`uniform` = equal, `distance` = closer neighbors have more influence).\n# * **`algorithm`** ‚Üí algorithm to compute nearest neighbors (`auto`, `ball_tree`, `kd_tree`, `brute`).\n# * **`p`** ‚Üí power parameter for the Minkowski distance (1 = Manhattan, 2 = Euclidean).\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the best KNN configuration.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:25.464071Z\",\"iopub.execute_input\":\"2025-10-18T09:47:25.464363Z\",\"iopub.status.idle\":\"2025-10-18T09:47:25.479281Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:25.464336Z\",\"shell.execute_reply\":\"2025-10-18T09:47:25.478255Z\"}}\nCV_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for the K-Nearest Neighbors model:\n# \n# * **`estimator=knn`** ‚Üí the KNN model to tune.\n# * **`param_grid=param_grid`** ‚Üí all combinations of hyperparameters to try.\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold stratified cross-validation, keeping the class proportions consistent.\n# \n# ‚úÖ **Purpose:** automatically find the **best KNN hyperparameters** while validating performance.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:25.480150Z\",\"iopub.execute_input\":\"2025-10-18T09:47:25.480418Z\",\"iopub.status.idle\":\"2025-10-18T09:47:28.667983Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:25.480393Z\",\"shell.execute_reply\":\"2025-10-18T09:47:28.667113Z\"}}\npipefinalknn = make_pipeline(col_trans, CV_knn)\npipefinalknn.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for the K-Nearest Neighbors model**:\n# \n# 1. **`col_trans`** ‚Üí applies preprocessing (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_knn`** ‚Üí runs GridSearchCV to find the **best KNN hyperparameters** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and KNN training happen together**, producing a ready-to-use model for predictions.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:28.668770Z\",\"iopub.execute_input\":\"2025-10-18T09:47:28.669034Z\",\"iopub.status.idle\":\"2025-10-18T09:47:28.673722Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:28.669017Z\",\"shell.execute_reply\":\"2025-10-18T09:47:28.672891Z\"}}\nprint(CV_knn.best_params_)\nprint(CV_knn.best_score_)\n\n# %% [markdown]\n# This code **displays the results of the KNN Grid Search**:\n# \n# * **`best_params_`** ‚Üí the combination of KNN hyperparameters that achieved the highest cross-validation score.\n# * **`best_score_`** ‚Üí the best average cross-validation accuracy using those parameters.\n# \n# ‚úÖ **Purpose:** check the **optimal KNN configuration** and its estimated performance.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:28.674601Z\",\"iopub.execute_input\":\"2025-10-18T09:47:28.674853Z\",\"iopub.status.idle\":\"2025-10-18T09:47:28.689502Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:28.674836Z\",\"shell.execute_reply\":\"2025-10-18T09:47:28.688746Z\"}}\nsvc = SVC(probability=True)\n\n# %% [markdown]\n# This line creates a **Support Vector Classifier (SVC)**:\n# \n# * **SVC** ‚Üí a model that finds the optimal hyperplane to separate classes.\n# * **`probability=True`** ‚Üí enables probability estimates for predictions (needed for ensemble voting or probability-based metrics).\n# \n# ‚úÖ **Purpose:** ready to train an SVM model to predict `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:28.690289Z\",\"iopub.execute_input\":\"2025-10-18T09:47:28.690517Z\",\"iopub.status.idle\":\"2025-10-18T09:47:28.705741Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:28.690499Z\",\"shell.execute_reply\":\"2025-10-18T09:47:28.704908Z\"}}\nparam_grid = {\n    'C': [100,10, 1.0, 0.1, 0.001, 0.001],\n    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n}\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning an SVM (Support Vector Classifier):\n# \n# * **`C`** ‚Üí regularization parameter controlling the trade-off between margin width and misclassification.\n# * **`kernel`** ‚Üí type of kernel function to use (`linear`, `poly`, `rbf`, `sigmoid`).\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the best SVM configuration for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:28.706603Z\",\"iopub.execute_input\":\"2025-10-18T09:47:28.706833Z\",\"iopub.status.idle\":\"2025-10-18T09:47:28.722081Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:28.706814Z\",\"shell.execute_reply\":\"2025-10-18T09:47:28.721168Z\"}}\nCV_svc = GridSearchCV(estimator=svc, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for the Support Vector Classifier (SVC):\n# \n# * **`estimator=svc`** ‚Üí the SVM model to tune.\n# * **`param_grid=param_grid`** ‚Üí all combinations of `C` and `kernel` to test.\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold cross-validation preserving class proportions.\n# \n# ‚úÖ **Purpose:** automatically find the **best SVM hyperparameters** while validating performance.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:47:28.722885Z\",\"iopub.execute_input\":\"2025-10-18T09:47:28.723254Z\",\"iopub.status.idle\":\"2025-10-18T09:51:21.703227Z\",\"shell.execute_reply.started\":\"2025-10-18T09:47:28.723223Z\",\"shell.execute_reply\":\"2025-10-18T09:51:21.702313Z\"}}\npipefinalsvc = make_pipeline(col_trans, CV_svc)\npipefinalsvc.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for the SVM model**:\n# \n# 1. **`col_trans`** ‚Üí applies preprocessing (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_svc`** ‚Üí runs GridSearchCV to find the **best SVM hyperparameters** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and SVM training happen together**, producing a ready-to-use SVM model for prediction.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:21.704192Z\",\"iopub.execute_input\":\"2025-10-18T09:51:21.704460Z\",\"iopub.status.idle\":\"2025-10-18T09:51:21.709231Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:21.704437Z\",\"shell.execute_reply\":\"2025-10-18T09:51:21.708505Z\"}}\nprint(CV_svc.best_params_)\nprint(CV_svc.best_score_)\n\n# %% [markdown]\n# This code **displays the results of the SVM Grid Search**:\n# \n# * **`best_params_`** ‚Üí the combination of `C` and `kernel` that gave the highest cross-validation score.\n# * **`best_score_`** ‚Üí the best average cross-validation accuracy achieved with those parameters.\n# \n# ‚úÖ **Purpose:** check the **optimal SVM configuration** and its estimated performance.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:21.710048Z\",\"iopub.execute_input\":\"2025-10-18T09:51:21.710280Z\",\"iopub.status.idle\":\"2025-10-18T09:51:21.727043Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:21.710258Z\",\"shell.execute_reply\":\"2025-10-18T09:51:21.726100Z\"}}\nlr = LogisticRegression()\n\n# %% [markdown]\n# This line creates a **Logistic Regression classifier**:\n# \n# * **LogisticRegression** ‚Üí a linear model that predicts the probability of a binary outcome (`Survived` vs `Not Survived`).\n# * Using **default parameters** here.\n# \n# ‚úÖ **Purpose:** ready to train a Logistic Regression model for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:21.727905Z\",\"iopub.execute_input\":\"2025-10-18T09:51:21.728272Z\",\"iopub.status.idle\":\"2025-10-18T09:51:21.743292Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:21.728245Z\",\"shell.execute_reply\":\"2025-10-18T09:51:21.742341Z\"}}\nparam_grid = {\n    'C': [100,10, 1.0, 0.1, 0.001, 0.001],\n}\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning Logistic Regression:\n# \n# * **`C`** ‚Üí inverse of regularization strength (smaller values ‚Üí stronger regularization, larger values ‚Üí weaker regularization).\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the best regularization parameter for the Logistic Regression model.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:21.744266Z\",\"iopub.execute_input\":\"2025-10-18T09:51:21.744572Z\",\"iopub.status.idle\":\"2025-10-18T09:51:21.759374Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:21.744545Z\",\"shell.execute_reply\":\"2025-10-18T09:51:21.758519Z\"}}\nCV_lr = GridSearchCV(estimator=lr, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for Logistic Regression:\n# \n# * **`estimator=lr`** ‚Üí the Logistic Regression model to tune.\n# * **`param_grid=param_grid`** ‚Üí tests different values of `C` (regularization strength).\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold stratified cross-validation, keeping class proportions.\n# \n# ‚úÖ **Purpose:** automatically find the **best regularization parameter** for Logistic Regression.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:21.760186Z\",\"iopub.execute_input\":\"2025-10-18T09:51:21.760493Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.523663Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:21.760467Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.522913Z\"}}\npipefinallr= make_pipeline(col_trans, CV_lr)\npipefinallr.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for Logistic Regression**:\n# \n# 1. **`col_trans`** ‚Üí applies preprocessing (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_lr`** ‚Üí runs GridSearchCV to find the **best Logistic Regression regularization (`C`)** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and Logistic Regression training happen together**, producing a ready-to-use model for prediction.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.524262Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.524490Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.531427Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.524471Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.528278Z\"}}\nprint(CV_lr.best_params_)\nprint(CV_lr.best_score_)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.532324Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.532739Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.544200Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.532714Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.543269Z\"}}\ngnb = GaussianNB()\n\n# %% [markdown]\n# This line creates a **Gaussian Naive Bayes classifier**:\n# \n# * **GaussianNB** ‚Üí a probabilistic model assuming **features are normally distributed** and independent.\n# * No hyperparameters need tuning by default.\n# \n# ‚úÖ **Purpose:** ready to train a Naive Bayes model for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.545525Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.545844Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.560620Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.545800Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.559663Z\"}}\nparam_grid = {\n    'var_smoothing': [0.00000001, 0.000000001, 0.00000001],\n}\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning Gaussian Naive Bayes:\n# \n# * **`var_smoothing`** ‚Üí a small value added to the variance to **stabilize calculations** and avoid division by zero.\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the best `var_smoothing` for the GaussianNB model.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.561530Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.561805Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.577466Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.561782Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.576550Z\"}}\nCV_gnb = GridSearchCV(estimator=gnb, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for Gaussian Naive Bayes:\n# \n# * **`estimator=gnb`** ‚Üí the Gaussian Naive Bayes model to tune.\n# * **`param_grid=param_grid`** ‚Üí tests different `var_smoothing` values.\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold stratified cross-validation to keep class proportions.\n# \n# ‚úÖ **Purpose:** automatically find the **best variance smoothing** for GaussianNB.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.578532Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.578774Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.714176Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.578755Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.712993Z\"}}\npipefinalgnb= make_pipeline(col_trans, CV_gnb)\npipefinalgnb.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for Gaussian Naive Bayes**:\n# \n# 1. **`col_trans`** ‚Üí applies preprocessing (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_gnb`** ‚Üí runs GridSearchCV to find the **best `var_smoothing`** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and GaussianNB training happen together**, producing a ready-to-use model for prediction.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.715296Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.715738Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.720377Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.715708Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.719489Z\"}}\nprint(CV_gnb.best_params_)\nprint(CV_gnb.best_score_)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.721196Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.721469Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.734658Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.721445Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.733815Z\"}}\nxg = XGBClassifier()\n\n# %% [markdown]\n# This line creates an **XGBoost classifier**:\n# \n# * **XGBClassifier** ‚Üí an optimized gradient boosting model that builds an ensemble of trees sequentially, improving performance by minimizing errors from previous trees.\n# * Uses default hyperparameters here.\n# \n# ‚úÖ **Purpose:** ready to train an XGBoost model for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.735629Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.736223Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.750951Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.736196Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.749929Z\"}}\nparam_grid = {\n     'booster': ['gbtree', 'gblinear','dart'],\n}\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning the XGBoost classifier:\n# \n# * **`booster`** ‚Üí type of boosting algorithm to use:\n# \n#   * `'gbtree'` ‚Üí tree-based boosting (default)\n#   * `'gblinear'` ‚Üí linear booster\n#   * `'dart'` ‚Üí tree booster with Dropouts\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the best booster type for the XGBoost model.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.751830Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.752781Z\",\"iopub.status.idle\":\"2025-10-18T09:51:28.767798Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.752755Z\",\"shell.execute_reply\":\"2025-10-18T09:51:28.767005Z\"}}\nCV_xg = GridSearchCV(estimator=xg, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for the XGBoost classifier:\n# \n# * **`estimator=xg`** ‚Üí the XGBoost model to tune.\n# * **`param_grid=param_grid`** ‚Üí tests different `booster` types (`gbtree`, `gblinear`, `dart`).\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold stratified cross-validation to maintain class proportions.\n# \n# ‚úÖ **Purpose:** automatically find the **best booster type** for XGBoost.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:28.768673Z\",\"iopub.execute_input\":\"2025-10-18T09:51:28.768927Z\",\"iopub.status.idle\":\"2025-10-18T09:51:32.083473Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:28.768894Z\",\"shell.execute_reply\":\"2025-10-18T09:51:32.082676Z\"}}\npipefinalxg= make_pipeline(col_trans, CV_xg)\npipefinalxg.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for XGBoost**:\n# \n# 1. **`col_trans`** ‚Üí applies preprocessing (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_xg`** ‚Üí runs GridSearchCV to find the **best booster type** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and XGBoost training happen together**, producing a ready-to-use model for prediction.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:32.084301Z\",\"iopub.execute_input\":\"2025-10-18T09:51:32.084530Z\",\"iopub.status.idle\":\"2025-10-18T09:51:32.088563Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:32.084511Z\",\"shell.execute_reply\":\"2025-10-18T09:51:32.087946Z\"}}\nprint(CV_xg.best_params_)\nprint(CV_xg.best_score_)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:32.089309Z\",\"iopub.execute_input\":\"2025-10-18T09:51:32.089707Z\",\"iopub.status.idle\":\"2025-10-18T09:51:32.104762Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:32.089685Z\",\"shell.execute_reply\":\"2025-10-18T09:51:32.104004Z\"}}\nabc = AdaBoostClassifier()\n\n# %% [markdown]\n# This line creates an **AdaBoost classifier**:\n# \n# * **AdaBoostClassifier** ‚Üí an ensemble method that combines multiple weak learners (usually decision stumps) sequentially, giving more weight to misclassified samples.\n# * Uses default parameters here.\n# \n# ‚úÖ **Purpose:** ready to train an AdaBoost model for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:32.105587Z\",\"iopub.execute_input\":\"2025-10-18T09:51:32.105968Z\",\"iopub.status.idle\":\"2025-10-18T09:51:32.120447Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:32.105925Z\",\"shell.execute_reply\":\"2025-10-18T09:51:32.119425Z\"}}\ndtc_2 = DecisionTreeClassifier(criterion = 'entropy', max_depth=10,min_samples_leaf=4, min_samples_split=10)  \nsvc_2 = SVC(probability=True, C=10, kernel='rbf') \nlr_2 = LogisticRegression(C=0.1) \nlr_3 = LogisticRegression(C=0.2) \nlr_4 = LogisticRegression(C=0.05) \n\n# %% [markdown]\n# This code **creates multiple tuned classifiers** with specific hyperparameters:\n# \n# * **`dtc_2`** ‚Üí Decision Tree with custom depth, split, and leaf settings.\n# * **`svc_2`** ‚Üí SVM with RBF kernel and regularization `C=10`, probability estimates enabled.\n# * **`lr_2`, `lr_3`, `lr_4`** ‚Üí Logistic Regression models with different regularization strengths (`C`).\n# \n# ‚úÖ **Purpose:** set up multiple **specific classifiers** for ensemble or comparative evaluation.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:32.121508Z\",\"iopub.execute_input\":\"2025-10-18T09:51:32.121820Z\",\"iopub.status.idle\":\"2025-10-18T09:51:32.136487Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:32.121791Z\",\"shell.execute_reply\":\"2025-10-18T09:51:32.135720Z\"}}\nparam_grid = {\n    'estimator': [dtc_2, svc_2, lr_2], \n    'n_estimators':  [5, 10, 25, 50, 100],\n    'algorithm': ['SAMME', 'SAMME.R'],\n    'learning_rate': [(0.97 + x / 100) for x in range(1, 7)]  \n}\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning an AdaBoost ensemble:\n# \n# * **`estimator`** ‚Üí base learners to use: `dtc_2`, `svc_2`, `lr_2`.\n# * **`n_estimators`** ‚Üí number of boosting rounds (5, 10, 25, 50, 100).\n# * **`algorithm`** ‚Üí boosting algorithm:\n# \n#   * `'SAMME'` ‚Üí discrete AdaBoost\n#   * `'SAMME.R'` ‚Üí real AdaBoost using probabilities\n# * **`learning_rate`** ‚Üí weights applied to each classifier; here `[0.9701, 0.9702, ..., 0.9706]`.\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the **best AdaBoost configuration**.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:32.137226Z\",\"iopub.execute_input\":\"2025-10-18T09:51:32.137451Z\",\"iopub.status.idle\":\"2025-10-18T09:51:32.151757Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:32.137429Z\",\"shell.execute_reply\":\"2025-10-18T09:51:32.150990Z\"}}\nCV_abc = GridSearchCV(estimator=abc, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for the AdaBoost classifier:\n# \n# * **`estimator=abc`** ‚Üí the AdaBoost model to tune.\n# * **`param_grid=param_grid`** ‚Üí tests different base estimators, number of estimators, algorithms, and learning rates.\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold stratified cross-validation to preserve class proportions.\n# \n# ‚úÖ **Purpose:** automatically find the **best combination of AdaBoost parameters and base learners**.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T09:51:32.152536Z\",\"iopub.execute_input\":\"2025-10-18T09:51:32.152768Z\",\"iopub.status.idle\":\"2025-10-18T10:14:16.928083Z\",\"shell.execute_reply.started\":\"2025-10-18T09:51:32.152750Z\",\"shell.execute_reply\":\"2025-10-18T10:14:16.927346Z\"}}\npipefinalabc= make_pipeline(col_trans, CV_abc)\npipefinalabc.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for AdaBoost**:\n# \n# 1. **`col_trans`** ‚Üí applies preprocessing (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_abc`** ‚Üí runs GridSearchCV to find the **best AdaBoost configuration** (base estimator, number of estimators, algorithm, learning rate) using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and AdaBoost training happen together**, producing a ready-to-use ensemble model.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:14:16.929012Z\",\"iopub.execute_input\":\"2025-10-18T10:14:16.929377Z\",\"iopub.status.idle\":\"2025-10-18T10:14:16.934606Z\",\"shell.execute_reply.started\":\"2025-10-18T10:14:16.929352Z\",\"shell.execute_reply\":\"2025-10-18T10:14:16.933597Z\"}}\nprint(CV_abc.best_params_)\nprint(CV_abc.best_score_)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:14:16.935454Z\",\"iopub.execute_input\":\"2025-10-18T10:14:16.936067Z\",\"iopub.status.idle\":\"2025-10-18T10:14:16.954061Z\",\"shell.execute_reply.started\":\"2025-10-18T10:14:16.936043Z\",\"shell.execute_reply\":\"2025-10-18T10:14:16.953223Z\"}}\netc = ExtraTreesClassifier()\n\n# %% [markdown]\n# This line creates an **Extra Trees classifier**:\n# \n# * **ExtraTreesClassifier** ‚Üí an ensemble of randomized decision trees; splits are chosen randomly, then averaged for prediction.\n# * Similar to Random Forest but usually **faster and more random**, reducing variance.\n# \n# ‚úÖ **Purpose:** ready to train an Extra Trees model for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:14:16.955002Z\",\"iopub.execute_input\":\"2025-10-18T10:14:16.955252Z\",\"iopub.status.idle\":\"2025-10-18T10:14:16.970364Z\",\"shell.execute_reply.started\":\"2025-10-18T10:14:16.955228Z\",\"shell.execute_reply\":\"2025-10-18T10:14:16.969588Z\"}}\nparam_grid = {\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"n_estimators\" :[100,300],\n}\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning the Extra Trees classifier:\n# \n# * **`max_features`** ‚Üí maximum number of features considered when splitting a node.\n# * **`min_samples_split`** ‚Üí minimum number of samples required to split a node.\n# * **`min_samples_leaf`** ‚Üí minimum number of samples required at a leaf node.\n# * **`n_estimators`** ‚Üí number of trees in the ensemble (100 or 300).\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the **best Extra Trees configuration**.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:14:16.971252Z\",\"iopub.execute_input\":\"2025-10-18T10:14:16.971609Z\",\"iopub.status.idle\":\"2025-10-18T10:14:16.986390Z\",\"shell.execute_reply.started\":\"2025-10-18T10:14:16.971584Z\",\"shell.execute_reply\":\"2025-10-18T10:14:16.985676Z\"}}\nCV_etc = GridSearchCV(estimator=etc, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for the Extra Trees classifier:\n# \n# * **`estimator=etc`** ‚Üí the Extra Trees model to tune.\n# * **`param_grid=param_grid`** ‚Üí tests different combinations of `max_features`, `min_samples_split`, `min_samples_leaf`, and `n_estimators`.\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold stratified cross-validation to maintain class proportions.\n# \n# ‚úÖ **Purpose:** automatically find the **best hyperparameters** for Extra Trees.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:14:16.987379Z\",\"iopub.execute_input\":\"2025-10-18T10:14:16.987609Z\",\"iopub.status.idle\":\"2025-10-18T10:15:24.761373Z\",\"shell.execute_reply.started\":\"2025-10-18T10:14:16.987593Z\",\"shell.execute_reply\":\"2025-10-18T10:15:24.760525Z\"}}\npipefinaletc= make_pipeline(col_trans, CV_etc)\npipefinaletc.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for Extra Trees**:\n# \n# 1. **`col_trans`** ‚Üí applies preprocessing (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_etc`** ‚Üí runs GridSearchCV to find the **best Extra Trees hyperparameters** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and Extra Trees training happen together**, producing a ready-to-use ensemble model.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:15:24.762257Z\",\"iopub.execute_input\":\"2025-10-18T10:15:24.762546Z\",\"iopub.status.idle\":\"2025-10-18T10:15:24.766620Z\",\"shell.execute_reply.started\":\"2025-10-18T10:15:24.762527Z\",\"shell.execute_reply\":\"2025-10-18T10:15:24.765808Z\"}}\nprint(CV_etc.best_params_)\nprint(CV_etc.best_score_)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:15:24.767397Z\",\"iopub.execute_input\":\"2025-10-18T10:15:24.767692Z\",\"iopub.status.idle\":\"2025-10-18T10:15:24.784122Z\",\"shell.execute_reply.started\":\"2025-10-18T10:15:24.767664Z\",\"shell.execute_reply\":\"2025-10-18T10:15:24.783199Z\"}}\nGBC = GradientBoostingClassifier()\n\n# %% [markdown]\n# This line creates a **Gradient Boosting classifier**:\n# \n# * **GradientBoostingClassifier** ‚Üí an ensemble method that builds trees sequentially, where each new tree **corrects the errors** of the previous ones.\n# * Uses default hyperparameters here.\n# \n# ‚úÖ **Purpose:** ready to train a Gradient Boosting model for predicting `Survived`.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:15:24.785471Z\",\"iopub.execute_input\":\"2025-10-18T10:15:24.785877Z\",\"iopub.status.idle\":\"2025-10-18T10:15:24.802066Z\",\"shell.execute_reply.started\":\"2025-10-18T10:15:24.785845Z\",\"shell.execute_reply\":\"2025-10-18T10:15:24.801130Z\"}}\nparam_grid = {\n              'n_estimators' : [300, 400, 500],\n              'learning_rate': [ 0.1, 0.3, 0.6, 1.0],\n              'max_depth': [8, 10, 12],\n              'min_samples_leaf': [50, 100, 120, 150],\n              'max_features': [0.1, 0.3, 0.5] \n              }\n\n# %% [markdown]\n# This code defines a **hyperparameter grid** for tuning the Gradient Boosting classifier:\n# \n# * **`n_estimators`** ‚Üí number of boosting stages (trees) to build.\n# * **`learning_rate`** ‚Üí shrinks the contribution of each tree; smaller values require more trees.\n# * **`max_depth`** ‚Üí maximum depth of each tree.\n# * **`min_samples_leaf`** ‚Üí minimum number of samples required at a leaf node.\n# * **`max_features`** ‚Üí fraction of features to consider when looking for the best split.\n# \n# ‚úÖ **Purpose:** used with **GridSearchCV** to find the **best Gradient Boosting configuration**.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:15:24.803028Z\",\"iopub.execute_input\":\"2025-10-18T10:15:24.803355Z\",\"iopub.status.idle\":\"2025-10-18T10:15:24.821341Z\",\"shell.execute_reply.started\":\"2025-10-18T10:15:24.803327Z\",\"shell.execute_reply\":\"2025-10-18T10:15:24.820455Z\"}}\nCV_gbc = GridSearchCV(estimator=GBC, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))\n\n# %% [markdown]\n# This code sets up **Grid Search with cross-validation** for the Gradient Boosting classifier:\n# \n# * **`estimator=GBC`** ‚Üí the Gradient Boosting model to tune.\n# * **`param_grid=param_grid`** ‚Üí tests combinations of `n_estimators`, `learning_rate`, `max_depth`, `min_samples_leaf`, and `max_features`.\n# * **`cv=StratifiedKFold(n_splits=5)`** ‚Üí 5-fold stratified cross-validation preserving class proportions.\n# \n# ‚úÖ **Purpose:** automatically find the **best hyperparameters** for Gradient Boosting.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:15:24.822161Z\",\"iopub.execute_input\":\"2025-10-18T10:15:24.822404Z\",\"iopub.status.idle\":\"2025-10-18T10:23:58.754375Z\",\"shell.execute_reply.started\":\"2025-10-18T10:15:24.822381Z\",\"shell.execute_reply\":\"2025-10-18T10:23:58.753492Z\"}}\npipefinalgbc= make_pipeline(col_trans, CV_gbc)\npipefinalgbc.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **creates and trains a full pipeline for Gradient Boosting**:\n# \n# 1. **`col_trans`** ‚Üí applies preprocessing (imputation, ordinal encoding, one-hot encoding, passthrough numeric features).\n# 2. **`CV_gbc`** ‚Üí runs GridSearchCV to find the **best Gradient Boosting hyperparameters** using 5-fold stratified cross-validation.\n# 3. **`fit(X_train, y_train)`** ‚Üí trains the entire pipeline on the training data.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and Gradient Boosting training happen together**, producing a ready-to-use model for prediction.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:23:58.755212Z\",\"iopub.execute_input\":\"2025-10-18T10:23:58.755425Z\",\"iopub.status.idle\":\"2025-10-18T10:23:58.760369Z\",\"shell.execute_reply.started\":\"2025-10-18T10:23:58.755408Z\",\"shell.execute_reply\":\"2025-10-18T10:23:58.759639Z\"}}\nprint(CV_gbc.best_params_)\nprint(CV_gbc.best_score_)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:23:58.761266Z\",\"iopub.execute_input\":\"2025-10-18T10:23:58.761511Z\",\"iopub.status.idle\":\"2025-10-18T10:23:58.776552Z\",\"shell.execute_reply.started\":\"2025-10-18T10:23:58.761488Z\",\"shell.execute_reply\":\"2025-10-18T10:23:58.775737Z\"}}\nvc1 = VotingClassifier([('gbc', CV_gbc.best_estimator_),\n                        ('etc', CV_etc.best_estimator_),\n                          ('nb', CV_gnb.best_estimator_)\n                         ], voting='hard', weights=[1,2,3] )\n\n# %% [markdown]\n# This code creates a **Voting Classifier ensemble** using the best models from previous Grid Searches:\n# \n# * **`VotingClassifier`** ‚Üí combines multiple models and predicts based on **majority vote** (`voting='hard'`).\n# * **`weights=[1,2,3]`** ‚Üí gives different importance to each model in the voting:\n# \n#   * `gbc` ‚Üí weight 1\n#   * `etc` ‚Üí weight 2\n#   * `nb` ‚Üí weight 3\n# * **`CV_gbc.best_estimator_`, `CV_etc.best_estimator_`, `CV_gnb.best_estimator_`** ‚Üí use the best-tuned models from GridSearchCV.\n# \n# ‚úÖ **Purpose:** create an **ensemble model** that leverages the strengths of multiple classifiers for better overall performance.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:23:58.777362Z\",\"iopub.execute_input\":\"2025-10-18T10:23:58.777616Z\",\"iopub.status.idle\":\"2025-10-18T10:23:58.796113Z\",\"shell.execute_reply.started\":\"2025-10-18T10:23:58.777591Z\",\"shell.execute_reply\":\"2025-10-18T10:23:58.795377Z\"}}\nvc2 = VotingClassifier([('abc', CV_abc.best_estimator_),\n                        ('etc', CV_etc.best_estimator_),\n                          ('nb', CV_gnb.best_estimator_)\n                         ], voting='hard', weights=[1,2,3] )\n#1,2,3 is the best performing one\n\n# %% [markdown]\n# This code creates a **second Voting Classifier ensemble** using different models:\n# \n# * **`VotingClassifier`** ‚Üí combines multiple classifiers and predicts based on **majority vote** (`voting='hard'`).\n# * **`weights=[1,2,3]`** ‚Üí assigns importance to each model based on performance:\n# \n#   * `abc` ‚Üí weight 1\n#   * `etc` ‚Üí weight 2\n#   * `nb` ‚Üí weight 3 (best performing model)\n# * **`CV_abc.best_estimator_`, `CV_etc.best_estimator_`, `CV_gnb.best_estimator_`** ‚Üí use the **best-tuned models** from previous GridSearchCVs.\n# \n# ‚úÖ **Purpose:** build an **optimized ensemble** that emphasizes the best-performing classifiers for higher accuracy.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:23:58.796886Z\",\"iopub.execute_input\":\"2025-10-18T10:23:58.797241Z\",\"iopub.status.idle\":\"2025-10-18T10:23:58.812889Z\",\"shell.execute_reply.started\":\"2025-10-18T10:23:58.797221Z\",\"shell.execute_reply\":\"2025-10-18T10:23:58.812000Z\"}}\npipefinalcv1 = make_pipeline(col_trans, vc1)\n\n# %% [markdown]\n# This code **creates a full pipeline combining preprocessing with the first Voting Classifier**:\n# \n# 1. **`col_trans`** ‚Üí applies all preprocessing steps (imputation, encoding, passthrough numeric features).\n# 2. **`vc1`** ‚Üí the first Voting Classifier ensemble that combines `GBC`, `ETC`, and `GNB` with weights `[1,2,3]`.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and ensemble prediction happen together**, making the pipeline ready to train or predict on new data.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:23:58.813682Z\",\"iopub.execute_input\":\"2025-10-18T10:23:58.813920Z\",\"iopub.status.idle\":\"2025-10-18T10:23:58.829454Z\",\"shell.execute_reply.started\":\"2025-10-18T10:23:58.813898Z\",\"shell.execute_reply\":\"2025-10-18T10:23:58.828420Z\"}}\npipefinalcv2 = make_pipeline(col_trans, vc2)\n\n# %% [markdown]\n# This code **creates a full pipeline combining preprocessing with the second Voting Classifier**:\n# \n# 1. **`col_trans`** ‚Üí applies all preprocessing steps (imputation, encoding, passthrough numeric features).\n# 2. **`vc2`** ‚Üí the second Voting Classifier ensemble that combines `ABC`, `ETC`, and `GNB` with weights `[1,2,3]`.\n# \n# ‚úÖ **Purpose:** ensures **preprocessing and ensemble prediction happen together**, making the pipeline ready to train or predict on new data.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:23:58.830318Z\",\"iopub.execute_input\":\"2025-10-18T10:23:58.830531Z\",\"iopub.status.idle\":\"2025-10-18T10:24:01.343963Z\",\"shell.execute_reply.started\":\"2025-10-18T10:23:58.830512Z\",\"shell.execute_reply\":\"2025-10-18T10:24:01.343179Z\"}}\npipefinalcv1.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **trains the full pipeline with the first Voting Classifier**:\n# \n# 1. **`col_trans`** ‚Üí preprocessing is applied to `X_train` (imputation, encoding, passthrough numeric features).\n# 2. **`vc1`** ‚Üí the first Voting Classifier ensemble is trained on the preprocessed data.\n# \n# ‚úÖ **Purpose:** ensures the **preprocessing and ensemble training happen together**, producing a ready-to-use model for predictions.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:24:01.344724Z\",\"iopub.execute_input\":\"2025-10-18T10:24:01.344984Z\",\"iopub.status.idle\":\"2025-10-18T10:24:02.918444Z\",\"shell.execute_reply.started\":\"2025-10-18T10:24:01.344964Z\",\"shell.execute_reply\":\"2025-10-18T10:24:02.917634Z\"}}\npipefinalcv2.fit(X_train, y_train)\n\n# %% [markdown]\n# This code **trains the full pipeline with the second Voting Classifier**:\n# \n# 1. **`col_trans`** ‚Üí preprocessing is applied to `X_train` (imputation, encoding, passthrough numeric features).\n# 2. **`vc2`** ‚Üí the second Voting Classifier ensemble is trained on the preprocessed data.\n# \n# ‚úÖ **Purpose:** ensures the **preprocessing and ensemble training happen together**, producing a ready-to-use model for predictions.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:24:02.919306Z\",\"iopub.execute_input\":\"2025-10-18T10:24:02.919584Z\",\"iopub.status.idle\":\"2025-10-18T10:24:03.247127Z\",\"shell.execute_reply.started\":\"2025-10-18T10:24:02.919564Z\",\"shell.execute_reply\":\"2025-10-18T10:24:03.246252Z\"}}\nY_pred = pipefinalrfc.predict(X_test)\nY_pred2 = pipefinaldtc.predict(X_test)\nY_pred3 = pipefinalknn.predict(X_test)\nY_pred4 = pipefinalsvc.predict(X_test)\nY_pred5 = pipefinallr.predict(X_test)\nY_pred6 = pipefinalgnb.predict(X_test)\nY_pred7 = pipefinalxg.predict(X_test)\nY_pred8 = pipefinalabc.predict(X_test)\nY_pred9 = pipefinaletc.predict(X_test)\nY_pred10 = pipefinalgbc.predict(X_test)\nY_pred11 = pipefinalcv1.predict(X_test)\nY_pred12 = pipefinalcv2.predict(X_test)\n\n# %% [markdown]\n# This code **generates predictions on the test set (`X_test`)** using all the trained pipelines and ensembles:\n# \n# * Each `Y_pred*` corresponds to predictions from a **different trained model or ensemble**:\n# \n#   1. `pipefinalrfc` ‚Üí Random Forest\n#   2. `pipefinaldtc` ‚Üí Decision Tree\n#   3. `pipefinalknn` ‚Üí KNN\n#   4. `pipefinalsvc` ‚Üí SVM\n#   5. `pipefinallr` ‚Üí Logistic Regression\n#   6. `pipefinalgnb` ‚Üí Gaussian Naive Bayes\n#   7. `pipefinalxg` ‚Üí XGBoost\n#   8. `pipefinalabc` ‚Üí AdaBoost\n#   9. `pipefinaletc` ‚Üí Extra Trees\n#   10. `pipefinalgbc` ‚Üí Gradient Boosting\n#   11. `pipefinalcv1` ‚Üí Voting Classifier 1\n#   12. `pipefinalcv2` ‚Üí Voting Classifier 2\n# \n# ‚úÖ **Purpose:** collect predictions from all models for **evaluation, comparison, or ensemble voting**.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:24:03.248220Z\",\"iopub.execute_input\":\"2025-10-18T10:24:03.248505Z\",\"iopub.status.idle\":\"2025-10-18T10:24:03.259542Z\",\"shell.execute_reply.started\":\"2025-10-18T10:24:03.248478Z\",\"shell.execute_reply\":\"2025-10-18T10:24:03.258529Z\"}}\nsubmission = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred\n})\n\nsubmission2 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred2\n})\n\nsubmission3 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred3\n})\n\nsubmission4 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred4\n})\n\nsubmission5 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred5\n})\n\nsubmission6 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred6\n})\n\nsubmission7 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred7\n})\n\nsubmission8 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred8\n})\n\nsubmission9 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred9\n})\n\nsubmission10 = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': Y_pred10\n})\n\nsubmission11 = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred11\n})\n\nsubmission12 = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred12\n})\n\n# %% [markdown]\n# This code **creates submission DataFrames for each model‚Äôs predictions** on the test set:\n# \n# * Each `submission*` corresponds to predictions from a **different trained model or ensemble**.\n# * **Columns:**\n# \n#   * `PassengerId` ‚Üí matches the test dataset IDs.\n#   * `Survived` ‚Üí predicted label (0 or 1) from the respective model.\n# \n# ‚úÖ **Purpose:** prepares **ready-to-submit DataFrames** for each model, e.g., for Kaggle Titanic competition.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-10-18T10:24:03.260361Z\",\"iopub.execute_input\":\"2025-10-18T10:24:03.260610Z\",\"iopub.status.idle\":\"2025-10-18T10:24:03.292784Z\",\"shell.execute_reply.started\":\"2025-10-18T10:24:03.260592Z\",\"shell.execute_reply\":\"2025-10-18T10:24:03.291951Z\"}}\nsubmission.to_csv('/kaggle/working/submission929_1.csv', index=False)\nsubmission2.to_csv('/kaggle/working/submission929_2.csv', index=False)\nsubmission3.to_csv('/kaggle/working/submission929_3.csv', index=False)\nsubmission4.to_csv('/kaggle/working/submission929_4.csv', index=False)\nsubmission5.to_csv('/kaggle/working/submission929_5.csv', index=False)\nsubmission6.to_csv('/kaggle/working/submission929_6.csv', index=False)\nsubmission7.to_csv('/kaggle/working/submission929_7.csv', index=False)\nsubmission8.to_csv('/kaggle/working/submission101_8.csv', index=False)\nsubmission9.to_csv('/kaggle/working/submission101_9.csv', index=False)\nsubmission10.to_csv('/kaggle/working/submission101_10.csv', index=False)\nsubmission11.to_csv('/kaggle/working/submission101_11.csv', index=False)\nsubmission12.to_csv('/kaggle/working/submission101_12.csv', index=False)\n\n# %% [markdown]\n# This code **saves all the submission DataFrames as CSV files** for submission to Kaggle:\n# \n# * **`to_csv()`** ‚Üí writes the DataFrame to a CSV file.\n# * **`index=False`** ‚Üí prevents pandas from writing row indices into the CSV.\n# * Each file corresponds to predictions from a **different trained model or ensemble**.\n# \n# ‚úÖ **Purpose:** prepare the model outputs in the **format required for Kaggle submission**.","metadata":{"_uuid":"b88ac0a6-3dad-4466-ba52-c6d7713d682e","_cell_guid":"271f1f08-81a4-4a56-98b8-ebbb5b0dc448","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}